---
layout: archive
title: "Seminar: AutoML in the Age of LLMs and Pre-trained Models"
collection: teaching
type: "Seminar"
permalink: /teaching/2024-summer-seminar
venue: "University of Tübingen"
date: 2024-04-22
location: "Tübingen"
---

Have you heard about AutoML? Do you wonder what LLMs can do for AutoML and vice versa? Then this seminar is for you!

**TL;DR** AutoML aims to support ML users (and ML researchers) by 
automating parts of the ML workflow. In this seminar we will read recent
research papers in the field of AutoML with a focus on methods for and with LLMs 
and pre-trained models. For a detailed overview, see this [paper](https://arxiv.org/abs/2306.08107).


| Course Title | AutoML in the Age of LLMs and Pre-trained Models                                                                                                                                                                                    |
|--------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Course ID    | ML4501f                                                                                                                                                                                                                             |
| Registration | [ILIAS](https://ovidius.uni-tuebingen.de/ilias3/goto.php?target=crs_4566638&client_id=pr02)                                                                                                                                         |
| ECTS         | 3                                                                                                                                                                                                                                   |
| Time         | Tuesdays, 12:15-13:45                                                                                                                                                                                                               |
| Language     | english                                                                                                                                                                                                                             |
| #participants | up to 12                                                                                                                                                                                                                            |
| Location     | in-person at [Maria-von-Linden-Straße 6](https://uni-tuebingen.de/einrichtungen/personalvertretungen-beratung-beauftragte/lageplaene/karte-c-sand-aussenbereiche-innenstadt/maria-von-linden-strasse-6/); lecture hall ground floor |

Why should you attend this seminar?
---
Besides practicing your scientific communication skills, you will also 
  * learn about key contributions in the field of AutoML
  * be able to discuss recent research on AutoML with and for LLMs/pre-trained models
  * gain experience in reading, understanding and presenting research papers 

Requirements
---
We strongly recommend that you know the foundations of machine learning and 
deep learning, including modern neural architectures including transformer models.
Ideally, you also have some experience in applying ML to get the most out of this seminar.

Topics
---
The seminar focuses on understanding the underlying concepts of modern
AutoML methods. Since this field is ever-growing, in this semester, 
we will focus on only a few topics as stated above.

Here is a *tentative* meeting schedule and a *tentative* paper list: 

| Date       | Content                                      |
|------------|----------------------------------------------|
| 16.04.2024 | Intro Seminar + Organization                 |
| 23.04.2024 | Intro AutoML I                               |
| 30.04.2024 | Intro AutoML II                              |
| 07.05.2024 | break                                        |
| 14.05.2024 | break                                        |
| 21.05.2024 | break                                        |
| 28.05.2024 | Bayesian Optimization (OptFormer;LLM4BO)     |
| 04.06.2024 | break                                        |
| 11.06.2024 | Tabular Data (TabPFN;XTab)                   |
| 18.06.2024 | Data Science (CAAFEE;MLAgent)                |
| 25.06.2024 | break                                        |
| 02.07.2024 | Neural Architecture Search (GPT4NAS;GPT-NAS) |
| 09.07.2024 | break                                        |
| 16.07.2024 | FineTuning (QuickTune;Bandits4LLMs)          |
| 23.07.2024 | ScalingLaws (TensorV;ScalingLaws4BO)         |


**LargeModels for AutoML**
1. [OptFormer] Chen et al. [Towards learning universal hyperparameter optimizers with transformers](https://papers.nips.cc/paper_files/paper/2022/hash/cf6501108fced72ee5c47e2151c4e153-Abstract-Conference.html) (NeurIPS’22)
2. [LLM4BO] Liu et al. [Large language models to enhance Bayesian optimization](https://openreview.net/forum?id=OOxotBmGol) (ICLR’24)
3. [TabPFN] Hollman et el. [TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second](https://openreview.net/forum?id=cp5PvcI6w8_) (ICLR’23)
4. [XTab] Zhu et al. [XTab: Cross-table Pretraining for Tabular Transformers](https://proceedings.mlr.press/v202/zhu23k.html) (ICML’23)
5. [GPT4NAS] Zheng et al. [Can GPT-4 Perform Neural Architecture Search?](https://arxiv.org/pdf/2304.10970.pdf) (arxiv’23)
6. [GPT-NAS] Yu et al. [GPT-NAS: Evolutionary Neural Architecture Search with the Generative Pre-Trained](https://arxiv.org/pdf/2305.05351.pdf) (arxiv’22)
7. [CAAFE] Hollman et al. [Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html) (NeurIPS’24)
8. [MLAgent] Huang et al. [Benchmarking Large Language Models as AI Research Agents](https://arxiv.org/abs/2310.03302) (arxiv’23)

**AutoML for Large Models**
1. [QuickTune] Arango et al. [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How](https://openreview.net/forum?id=tqh1zdXIra) (ICLR’24)
2. [Bandits4LLMs] Xia et al. [Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits](https://arxiv.org/pdf/2403.07213v1.pdf) (WWW’24)
3. [TensorV] Yang et al. [Tuning large neural networks via zero-shot hyperparameter transfer](https://papers.nips.cc/paper/2021/hash/8df7c2e3c3c3be098ef7b382bd2c37ba-Abstract.html) (NeurIPS’21)
4. [ScalingLaws4BO] Kadra et al. [Scaling Laws for Hyperparameter Optimization](https://proceedings.neurips.cc/paper_files/paper/2023/file/945c781d7194ea81026148838af95af7-Paper-Conference.pdf) (Neurips’23)




How the seminar will look like?
---

We will meet each week (with a few exceptions). In the first few weeks, we will start with introductory lectures on automated machine learning, Bayesian optimization, neural architecture search and how to critically review and present research papers. After that, each week, we will have presentations, followed by discussions.

Other Important information
---

**Registration:** Please register on ILIAS. The number of participants is limited and the registration opens on March 29th, noon. 
There will be a waiting list (please unregister to let other people take your place). 
Please come to the first lecture even if you are still on the waiting list. If you're enrolled and don't show up, your spot will be freed for someone on the waiting list.

**Grading/Presentations:** Grades will be based on your presentation, slides, active participation and a short report. Further details will be discussed in the intro session.


 

